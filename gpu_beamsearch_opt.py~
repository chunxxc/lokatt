import numpy as np
import tensorflow as tf
import argparse
import sys
import tensorflow_probability as tfp
import h5py
import time
import threading
import hdf5plugin
from utils_dna import prepro_signal,base2num,seg_assembler
from tqdm import tqdm
#from model_2RES2BI512_resoriginal import Model_RESBi
from model_2RES2BI512_resoriginal_smallk import Model_RESBi
from model_unet import Model_UNET
import time
# parse keyboard inputs
def str2bool(v):
  if v.lower() in ('yes', 'True','true', 't', 'y', '1'):
    return True
  elif v.lower() in ('no','Flase', 'false', 'f', 'n', '0'):
    return False
  else:
    raise argparse.ArgumentTypeError('Boolean value expected.')
def parse_arguments(sys_input):
  parser = argparse.ArgumentParser()
  parser.add_argument('-output',default='./',type=str,help='where to save fasta file')
  #parser.add_argument('-model',default='2RES2BI512_4096',help='which NN structure to use, choose between \'2RES2BI512_4096\', \'resorigin\',\'prorelu\',\'resconvskip\',\'resnoskiprelu\' ')
  parser.add_argument('-tf_weights','--tf_weights_path',default='',type=str,help='TF weights path')
  parser.add_argument('-kmer','--K',type = int,default = 5, help='specify kmer in model init, default 5')
  parser.add_argument('-name',default='_beamsearch',type=str,help='fasta name') 
  parser.add_argument('-d','--device',default=0,type=int,help='which gpu to use, default 0')
  parser.add_argument('-batch',type=int,default=60,help='batch size, default 60')
  parser.add_argument('-max_reads',type=int,default=-1,help='max reads, default all')
  parser.add_argument('-fast5','--fast5_path',default='./',type=str,help='input raw reads file')
  parser.add_argument('-ll',type=str2bool,default=True,help='if use Loglogistic duration estimation per read, default True')
  parser.add_argument('-norm_sig',type=str2bool,default=True,help='if normalize signals, default true')
  
  parser.add_argument('-seg_length',type = int,default = 4096, help='length of signals used for one viterbi,default 4096')
  parser.add_argument('-stride',type=int,default=3800,help='stride for signal segments,default 3800, if set to -1 then no seg is used')
  return parser.parse_args(sys_input)
def get_params():
  PARAMS = parse_arguments(sys.argv[1:])
  if not PARAMS.output[-1]=='/':
    PARAMS.output += '/'
  return PARAMS
###########################################################
def write_fasta(f,read_id,read_bases):
    f.write('>'+read_id+'\n')
    for i in range(len(read_bases)//80+1):
        f.write(read_bases[i*80:(i+1)*80])
        f.write('\n')
def means_windowing(x,size=4):
  x = np.array(x)
  x_ = np.zeros((x.size-size+1,size))
  for i in range(size-1):
    x_[:,i] = x[i:-size+i+1]
  x_[:,-1] = x[size-1:]
  means = np.median(x_,axis=1)
  return means
def diffs(x):
  x = np.array(x)
  diff = list()
  for i in range(len(x)-1):
    diff.append(np.abs(x[i+1]-x[i]))
  diff = np.array(diff)
  return diff
def estimate_mu(signals,window_size=4,diff_thre=0.4):
  signals = signals.reshape(-1)
  sig_means = means_windowing(signals,window_size)#median 
  differences = diffs(sig_means)
  duration = len(signals)/np.sum(differences>diff_thre)
  if duration > 20:
    duration = 20
  mu = 0.0927*duration+1.1454
  return mu
#
def finish_fasta(output,batch_id,finished_id,f_output,lock):
  global total_bases
  global base_dict
  for i in range(output.shape[0]):
    bases = output[i,output[i,:]>=0]
    bases = base2num(bases.tolist(),PARAMS.K,1,1)
    if len(bases)==0:
      continue
    temp_base = bases[0]
    for j in range(1,len(bases)):
      temp_base += bases[j][-1]
    bases_dict[batch_id[i]].append(temp_base)
  for i in range(len(finished_id)):
    read_bases = seg_assembler(bases_dict[finished_id[i]],p=assembler_percent)
    total_bases += len(read_bases)
    read_id = finished_id[i][5:]
    del bases_dict[finished_id[i]]
    with lock:
      f_output.write('>'+read_id+'\n')
      for i in range(len(read_bases)//80+1):
        f_output.write(read_bases[i*80:(i+1)*80])
        f_output.write('\n')

  
##########################################################
PARAMS = get_params()
#transition_probability = np.full((4, 4096), 0.25, dtype=np.single)
transition_probability = np.load('/home/chunx/DNA-NN/transition_5mer_ecoli.npy').astype(np.float32)
batch_size = PARAMS.batch
seg_length = np.int32(PARAMS.seg_length)
stride = np.int32(PARAMS.stride)
assembler_percent = stride/seg_length
physical_devices = tf.config.list_physical_devices('GPU')
tf.config.set_visible_devices(physical_devices[PARAMS.device], 'GPU')
if PARAMS.K==3:
  dnaseq_beam_module = tf.load_op_library('/home/chunx/DNA-NN/dnaseq_beam_k3t3.so')
  transition_probability = np.full((4, 4**3), 0.25, dtype=np.single)
else:
  dnaseq_beam_module = tf.load_op_library('/home/chunx/DNA-NN/dnaseq_beam.so')
duration = np.zeros([batch_size,16],dtype=np.float32)
tail_factor = np.zeros([batch_size],dtype=np.float32)
base_per_read = 0
max_reads = PARAMS.max_reads
batch_duration = np.zeros((batch_size,16)).astype(np.float32)
batch_tail = np.zeros(batch_size).astype(np.float32)
batch_input = np.zeros((batch_size,PARAMS.seg_length,1)).astype(np.float32)-100
batch_id = []
finished_id = []
bases_dict = {}
total_bases = 0
lock = threading.Lock()
threads = []
for i in range(10):
  threads.append(threading.Thread())
for thre in threads:
  thre.start()
### load fast5 and prepare output file
if '.fast5' in PARAMS.fast5_path[-7:] or 'f5' in PARAMS.fast5_path[-3:]:
  out_fn = PARAMS.fast5_path[PARAMS.fast5_path.rfind('/')+1:PARAMS.fast5_path.rfind('.fast5')]
  fname = PARAMS.output+out_fn
  f = h5py.File(PARAMS.fast5_path,'r')
  ###
  import pdb;pdb.set_trace()
  mus = []
  for node_name, node in tqdm(f.items()):
    #pbar.update(1)
    #tqdm.write('\r collecting signals...')
    bases_dict[node_name] = []
    #threads.append(threading.Thread(target=beam_search_read,args=(node_name,node,fname,lock,)))
    norm_signals = prepro_signal(node['Raw']['Signal'][:]) # no quantized signal
    if PARAMS.ll:
      nowt = time.time()
      if len(norm_signals)>4000:
        mu = estimate_mu(norm_signals)
      else:
        mu = 2.1
    mus.append(mu)
  import pdb;pdb.set_trace()
'''
  if 'unet' in PARAMS.tf_weights_path:
    myNN = Model_UNET(kmer=PARAMS.K)
  else:
    myNN = Model_RESBi(kmer=PARAMS.K)
  test = myNN(np.random.rand(1,4096,1).astype(np.float32))
  myNN.load_weights(PARAMS.tf_weights_path)
  tf_weights_name = PARAMS.name
  print('loaded tf weights at'+PARAMS.tf_weights_path)
  fname += tf_weights_name
  if PARAMS.ll:
    fname += '_llduration_op.fasta'
  else:
    fname += '_duration6_op.fasta'
  f_output = open(fname,'w')
  count = 0
nodes = iter(f.items())
pbar = tqdm(total=max_reads)
start = time.time()
l_duration = np.zeros(16,dtype=np.float32)
l_tail = np.zeros(1,dtype=np.float32)
ll=tfp.distributions.LogLogistic(2.1,0.415)
ll_prob = ll.prob(np.arange(1,20).astype(np.float32))
l_duration[:15] = ll_prob[:15]
l_tail[:] = ll_prob[16]/ll_prob[15]
l_duration[-1] = ll_prob[15]/(1-l_tail)
cpu_t = threading.Thread()
cpu_t.start()
tf_trans = tf.constant(transition_probability)
for node_name, node in f.items():
    pbar.update(1)
    #tqdm.write('\r collecting signals...')
    bases_dict[node_name] = []
    #threads.append(threading.Thread(target=beam_search_read,args=(node_name,node,fname,lock,)))
    norm_signals = prepro_signal(node['Raw']['Signal'][:]) # no quantized signal
    if PARAMS.ll:
      nowt = time.time()
      if len(norm_signals)>4000:
        mu = estimate_mu(norm_signals)
      else:
        mu = 2.1
      ll=tfp.distributions.LogLogistic(mu,0.415)
      ll_prob = ll.prob(np.arange(1,20).astype(np.float32))
      l_duration[:15] = ll_prob[:15]
      l_tail[:] = ll_prob[16]/ll_prob[15]
      l_duration[-1] = ll_prob[15]/(1-l_tail)
    final_n = int(norm_signals.shape[0]//stride)+1 # we take whatever is at last
    for seg_i in range(final_n):
      #print('collecting signals')
      batch_id.append(node_name)
      batch_seg_i = len(batch_id)-1
      batch_duration[batch_seg_i,:] = l_duration[:]
      batch_tail[batch_seg_i] = l_tail[:]
      if seg_i*stride+seg_length > norm_signals.shape[0]:
        sig_seg_len = norm_signals.shape[0]-seg_i*stride
        batch_input[batch_seg_i,:sig_seg_len,0] = norm_signals[seg_i*stride:]# whatever is at last
      else:
        batch_input[batch_seg_i,:,0] = norm_signals[seg_i*stride:seg_i*stride+seg_length]
      if len(batch_id)==batch_size:
        #print('start NN')
        Px = myNN(batch_input)
        #print('start BS')
        output = dnaseq_beam_module.dnaseq_beam(Px+1e-5, tf.Variable(batch_duration), tf.Variable(batch_tail),tf_trans)
        #base_lens = tf.math.count_nonzero(output+1,axis=1)
        #output_cut = output[:,:tf.reduce_max(base_lens)]
        #print('waiting for cput')
        
        for thre_i in range(len(threads)):
          if not threads[thre_i].is_alive():
            threads[thre_i] = threading.Thread(target=finish_fasta,args=(output.numpy(),batch_id[:],finished_id[:],f_output,lock))
            threads[thre_i].start()
            break
        alive_T = 0
        for thre in threads:
          if thre.is_alive():
            alive_T += 1
        #tqdm.write(str(alive_T)+' streads running at once')
        count += len(finished_id)
        if count%100==0:
          tqdm.write('basecalling speed '+str(total_bases/(time.time()-start))+' bases/sec')
          total_bases = 0
          start=time.time()
        if max_reads>0 and count > max_reads:
          import pdb;pdb.set_trace()
        finished_id = []
        batch_id = []
        batch_input[:] = -100
        #tqdm.write('\r translate bases and assembler takes: '+str(time.time()-temp_start)+' sec')
    finished_id.append(node_name)
'''
